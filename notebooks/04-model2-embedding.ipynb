{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interesting-wisdom",
   "metadata": {},
   "source": [
    "### 1. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "immediate-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import spacy \n",
    "from spacy.lang.en import English\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "tutorial-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "settled-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Glove embeddings\n",
    "# ! wget http://nlp.stanford.edu/data/glove.6B.zip data/glove.6B.zip\n",
    "\n",
    "# ! unzip glove.6B.zip -d /home/jupyter/sb-entity-classification/data/glove.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pursuant-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/jupyter/sb-entity-classification/data/'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civilian-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appointed-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/jupyter/sb-entity-classification/data/data.csv')\n",
    "df.columns = ['class','name']\n",
    "\n",
    "classes_list = pd.read_csv('/home/jupyter/sb-entity-classification/data/classes.txt', header = None)\n",
    "classes_list['class'] = classes_list.index\n",
    "classes_list.columns = ['class_name', 'class']\n",
    "classes_list['class'] = classes_list['class'] + 1  # based on information provided in the brief\n",
    "\n",
    "df = df.merge(classes_list, on = 'class', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-mailing",
   "metadata": {},
   "source": [
    "### 2. Clean, Tokenize, and Pad Texts in Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sonic-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_cleaned'] = df['name'].str.lower()\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x)) # only english words for now\n",
    "\n",
    "stopwords = set(stopwords.words())\n",
    "df['name_cleaned'] = df['name_cleaned'].apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dying-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 291320 unique tokens.\n",
      "Max number of words in a name is 16\n"
     ]
    }
   ],
   "source": [
    "texts = df['name_cleaned']\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "\n",
    "max_seq_length = df['name_cleaned'].apply(lambda x: len(x.split(' '))).max()\n",
    "print('Max number of words in a name is {}'.format(max_seq_length))\n",
    "emb_texts = pad_sequences(sequences, maxlen=max_seq_length)\n",
    "df['name_seq'] = emb_texts.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "boolean-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors..\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors..')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(EMBEDDING_DIM))) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-special",
   "metadata": {},
   "source": [
    "### 3. Prepare GloVe Embedding dictionary for tokens that have appeared in names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "grand-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "skilled-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared embedding matrix\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Prepared embedding matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "opposite-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7354855011281862"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.euclidean(embedding_matrix[1],embedding_matrix[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-permit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lonely-three",
   "metadata": {},
   "source": [
    "To try: universal sentence encoder\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
