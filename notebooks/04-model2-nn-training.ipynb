{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honey-alignment",
   "metadata": {},
   "source": [
    "# 04 - model2 - neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-adjustment",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "linear-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "green-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/jupyter/sb-entity-classification/data/data.csv')\n",
    "df.columns = ['class','name']\n",
    "df['class'] = df['class'] -1  # based on information provided in the brief\n",
    "\n",
    "classes_list = pd.read_csv('/home/jupyter/sb-entity-classification/data/classes.txt', header = None)\n",
    "classes_list['class'] = classes_list.index\n",
    "classes_list.columns = ['class_name', 'class']\n",
    "class_names = classes_list['class_name'].tolist()\n",
    "df = df.merge(classes_list, on = 'class', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-spare",
   "metadata": {},
   "source": [
    "### 2. Split train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "involved-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train validation and test\n",
    "msk = np.random.rand(len(df)) < 0.98\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disabled-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle('/home/jupyter/sb-entity-classification/data/test_nn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "parliamentary-dairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537831, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dependent-fisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10956, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "violent-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train['name'].tolist()\n",
    "labels  = train['class'].tolist()\n",
    "\n",
    "# Shuffle the data\n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesbian-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-eclipse",
   "metadata": {},
   "source": [
    "### 3. Use pre-trained GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "running-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=8)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mighty-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "basic-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"sb-entity-classification/data/glove.6B/glove.6B.100d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "italic-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 19028 words (972 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unlimited-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-buyer",
   "metadata": {},
   "source": [
    "### 4. Build NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "explicit-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "earlier-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2000200   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         12928     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         16512     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 2,064,470\n",
      "Trainable params: 64,270\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(1)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(1)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "heard-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = callbacks.EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    min_delta=0,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "cbs = [\n",
    "    early_stopping_cb\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-sport",
   "metadata": {},
   "source": [
    "### 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caring-charge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3362/3362 [==============================] - 274s 82ms/step - loss: 1.0609 - acc: 0.6240 - val_loss: 0.9996 - val_acc: 0.6425\n",
      "Epoch 2/20\n",
      "3362/3362 [==============================] - 273s 81ms/step - loss: 0.9913 - acc: 0.6468 - val_loss: 0.9884 - val_acc: 0.6483\n",
      "Epoch 3/20\n",
      "3362/3362 [==============================] - 275s 82ms/step - loss: 0.9933 - acc: 0.6465 - val_loss: 0.9997 - val_acc: 0.6475\n",
      "Epoch 4/20\n",
      "3362/3362 [==============================] - 271s 81ms/step - loss: 1.0025 - acc: 0.6456 - val_loss: 1.0269 - val_acc: 0.6438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd4afd78390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"rmsprop\", \n",
    "              metrics=[\"acc\"])\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          batch_size=128, \n",
    "          epochs=20, \n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "completed-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "superior-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save models and artifacts\n",
    "# tf.keras.models.save_model(model, '/home/jupyter/sb-entity-classification/models/nn_embedding.h5')\n",
    "# tf.keras.models.save_model(end_to_end_model, '/home/jupyter/sb-entity-classification/models/end_to_end_nn_embedding')\n",
    "# with open('/home/jupyter/sb-entity-classification/models/class_names.txt', \"wb\") as fp:   \n",
    "#     pickle.dump(class_names, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-quilt",
   "metadata": {},
   "source": [
    "### 6. Predict using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "excessive-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(string_input):\n",
    "    probabilities = end_to_end_model.predict([[string_input]])\n",
    "    return class_names[np.argmax(probabilities[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-maria",
   "metadata": {},
   "source": [
    "#### 6.1 try it on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "controlled-explorer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407878</th>\n",
       "      <td>10</td>\n",
       "      <td>Diplosphaera</td>\n",
       "      <td>Plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139140</th>\n",
       "      <td>3</td>\n",
       "      <td>Anthony Ashley-Cooper (cricketer)</td>\n",
       "      <td>Athlete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236690</th>\n",
       "      <td>5</td>\n",
       "      <td>Jeep Grand Cherokee</td>\n",
       "      <td>MeanOfTransportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499752</th>\n",
       "      <td>12</td>\n",
       "      <td>Wombling Free</td>\n",
       "      <td>Film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413550</th>\n",
       "      <td>10</td>\n",
       "      <td>Ulmus 'Folia Variegata Pendula'</td>\n",
       "      <td>Plant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                               name            class_name\n",
       "407878     10                       Diplosphaera                 Plant\n",
       "139140      3  Anthony Ashley-Cooper (cricketer)               Athlete\n",
       "236690      5                Jeep Grand Cherokee  MeanOfTransportation\n",
       "499752     12                      Wombling Free                  Film\n",
       "413550     10    Ulmus 'Folia Variegata Pendula'                 Plant"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "greater-sussex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Animal'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"Diplosphaera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "legendary-symposium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Athlete'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"Anthony Ashley-Cooper (cricketer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affected-ambassador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MeanOfTransportation'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"Jeep Grand Cherokee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "collective-grave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Album'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"Wombling Free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "professional-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Plant'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(\"Ulmus 'Folia Variegata Pendula'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-convention",
   "metadata": {},
   "source": [
    "_Observations_:\n",
    "\n",
    "From the sample examples, the model is not able to differentiate titles for different media of artworks (when a keyword is absent), which is probably only going to be able to resolved or improved with additional data sources of contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-serum",
   "metadata": {},
   "source": [
    "#### 6.2 try it on a sample of test set to estimate run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "equipped-blocking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg time per prediction 0.19s\n",
      "Total time for all 100 forecasts: 0.01h\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "test_sample = test.sample(sample_size)\n",
    "start = time.time()\n",
    "predictions = list(map(get_prediction,test_sample['name'].tolist()))\n",
    "time_taken = time.time() - start\n",
    "avg_time_p = time_taken / sample_size\n",
    "print('Avg time per prediction {:.2f}s'.format(avg_time_p))\n",
    "print('Total time for all {} forecasts: {:.2f}h'.format(sample_size, time_taken / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "judicial-mission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Album      0.700     0.778     0.737         9\n",
      "                Animal      0.360     1.000     0.529         9\n",
      "                Artist      0.400     0.400     0.400         5\n",
      "               Athlete      0.286     0.286     0.286         7\n",
      "              Building      1.000     0.833     0.909         6\n",
      "               Company      1.000     0.500     0.667         8\n",
      "EducationalInstitution      0.833     1.000     0.909         5\n",
      "                  Film      0.833     0.714     0.769         7\n",
      "  MeanOfTransportation      1.000     0.857     0.923         7\n",
      "          NaturalPlace      1.000     1.000     1.000         6\n",
      "          OfficeHolder      1.000     0.200     0.333         5\n",
      "                 Plant      1.000     0.667     0.800        15\n",
      "               Village      1.000     0.400     0.571         5\n",
      "           WrittenWork      0.857     1.000     0.923         6\n",
      "\n",
      "              accuracy                          0.700       100\n",
      "             macro avg      0.805     0.688     0.697       100\n",
      "          weighted avg      0.807     0.700     0.706       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_sample['class_name'].tolist()[:sample_size], predictions, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-commodity",
   "metadata": {},
   "source": [
    "#### 6.3 to score on the entire test set:\n",
    "Please see scripts/score_with_nn.py, which was developed using notebooks/05-model2-nn-scoring.ipynb"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
