






Comments on methods that have been considered but not chosen to implement:

1. transformer-based models: 
Recent advances in NLP have been fueled by contextual language models such as transformers, ELMO, BERT, and the ever more powerful generations of GPT. Since these models are typically useful when contexts are abundant, i.e. when sentences and paragraphs are provided, rather than list of words, as is the case in this challenge,  I've narrowed down my approaches to more old-school but efficient methods reliant on word vectors. For 'future phase' of this 

2. 